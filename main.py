# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15TbRxS_C0YUL42d5gEG10jS23M7-DmGN

# Code for Deep Learning for Fashion: Performance Evaluation of Random Forests and CNNs for Fashion Image Classification

Please run this file after downloading the model in the github page and replacing it with the model path. The saved model should give you the classification summary details, as well as other relevent metrics.
"""

# Import necessary packages
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, log_loss,average_precision_score,matthews_corrcoef, hamming_loss
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
from itertools import cycle

# We load the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images,train_labels), (test_images,test_labels) = fashion_mnist.load_data()

# We load the saved model
model_path = 'cnn_model_v4.h5'  # Replace with your actual saved model path
loaded_model = tf.keras.models.load_model(model_path)

# We expand dimensions to add channel axis
X_expand_train = np.expand_dims(train_images, -1)
X_expand_test = np.expand_dims(test_images, -1)

# We evaluate on training data
score_train = loaded_model.evaluate(X_expand_train, train_labels, verbose = 0)
print('The Traning Metrics are: ')
print("Training Loss: {:.4f}".format(score_train[0]))
print("Training Accuracy: {:.4f}".format(score_train[1]))

# We evaluate on test data
score_test = loaded_model.evaluate(X_expand_test, test_labels, verbose = 0)
print('The Testing Metrics are: ')
print("Test Loss: {:.4f}".format(score_test[0]))
print("Test Accuracy: {:.4f}".format(score_test[1]))

# We predict labels for test data
y_test_pred = loaded_model.predict(X_expand_test)
y_test_pred_classes = np.argmax(y_test_pred, axis = 1)

# We generate classification report
print("Classification Report:")
print(classification_report(test_labels, y_test_pred_classes, digits = 4))

# We generate confusion matrix
conf_matrix = confusion_matrix(test_labels, y_test_pred_classes)
plt.figure(figsize = (5, 5))
sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Blues', cbar = False)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# We identify misclassified samples
misclassified_indices = np.where(y_test_pred_classes != test_labels)[0]

print(f"Number of misclassified samples: {len(misclassified_indices)}")

# We visualize some misclassified examples
plt.figure(figsize=(7, 7))
for i, idx in enumerate(misclassified_indices[:16]):
    plt.subplot(4, 4, i + 1)
    plt.imshow(test_images[idx], cmap="gray")
    true_label = class_names[test_labels[idx]]
    pred_label = class_names[y_test_pred_classes[idx]]
    plt.title(f"True: {true_label}\nPred: {pred_label}", fontsize = 10)
    plt.axis("off")
plt.tight_layout()
plt.show()

# We obtain binarize labels for multi-class ROC
n_classes = 10
y_test_binarized = label_binarize(test_labels, classes=np.arange(n_classes))

# We compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_test_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot the ROC curve
plt.figure(figsize=(5, 3))
colors = cycle(["aqua", "darkorange", "cornflowerblue", "green", "red", "purple", "brown", "pink", "gray", "olive"])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color = color, lw = 2,
             label="ROC curve of class {0} (area = {1:0.2f})".format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], "k--", lw = 2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic for Multi-Class")
plt.legend(loc="lower right")
plt.show()

# We calculate logarithmic loss
logloss = log_loss(test_labels, y_test_pred)
print("The logarithmic loss is: {:.4f}".format(logloss))

# We calculate Matthews Correlation Coefficient (MCC) https://plos.figshare.com/articles/figure/_Calculation_of_Matthews_Correlation_Coefficient_MCC_/329908?file=659418
mcc = matthews_corrcoef(test_labels, y_test_pred_classes)
print(f"The Matthews Correlation Coefficient is: {mcc:.4f}")

# We calculate Hamming Loss https://medium.datadriveninvestor.com/a-survey-of-evaluation-metrics-for-multilabel-classification-bb16e8cd41cd
hamming = hamming_loss(test_labels, y_test_pred_classes)
print(f"The Hamming Loss is: {hamming:.4f}")

# We calculate Area Under the Precision-Recall Curve (PR AUC)
pr_auc = average_precision_score(y_test_binarized, y_test_pred, average="macro")
print(f"The PR AUC is: {pr_auc:.4f}")